import json
import requests
import re
import logging
from django.conf import settings
from celery import shared_task
from django.utils import timezone
from .models import Tribute

logger = logging.getLogger(__name__)

# –ê–Ω–∞–ª–∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏–º–µ–Ω–∏ –º–µ–º–æ—Ä–∏–∞–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ
def analyze_name_mentions(text, memorial):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–º–µ–Ω–∏ –º–µ–º–æ—Ä–∏–∞–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞.
    """
    text_lower = text.lower()
    full_name = f"{memorial.first_name} {memorial.last_name}".lower()
    first_name = memorial.first_name.lower()
    last_name = memorial.last_name.lower()
    
    results = {
        'full_name_mentioned': full_name in text_lower,
        'first_name_mentioned': first_name in text_lower,
        'last_name_mentioned': last_name in text_lower,
        'other_names_found': [],
        'wrong_first_name_detected': False,
        'wrong_last_name_detected': False,
        'context': 'unknown'
    }

    # –®–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ "–ò–º—è –§–∞–º–∏–ª–∏—è" –∏–ª–∏ "–§–∞–º–∏–ª–∏—è"
    name_patterns = [
        r'\b([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)\s+([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)\b',  
        r'\b(Herr|Frau|Mr\.|Mrs\.|Ms\.)\s+([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+(?:\s+[A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)?)\b',
    ]
    
    all_name_matches = []
    for pattern in name_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                # –î–ª—è "–ò–º—è –§–∞–º–∏–ª–∏—è"
                name_parts = [m for m in match if m and len(m) > 1]
                if len(name_parts) >= 2:
                    found_name = ' '.join(name_parts[:2])
                    all_name_matches.append(found_name)
            else:
                # –î–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                if match and len(match) > 2:
                    all_name_matches.append(match)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–∞
    found_names = set()
    for name in all_name_matches:
        name_lower = name.lower()
        
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞
        ignore_words = { # –û–±—Ä–∞—â–µ–Ω–∏—è
            'herr', 'frau', 'mr', 'mrs', 'ms', 'fraulein', 'dr', 'prof',
            # –°–µ–º—å—è/–æ—Ç–Ω–æ—à–µ–Ω–∏—è
            'family', 'familie', 'and', 'und', 'oder', 'or',
            # –ê—Ä—Ç–∏–∫–ª–∏/–º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è
            'der', 'die', 'das', 'den', 'dem', 'des',
            'sein', 'seine', 'seinem', 'seinen', 'seiner',
            'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer',
            'unser', 'unsere', 'unserem', 'unseren', 'unserer',
            'euer', 'eure', 'eurem', 'euren', 'eurer',
            # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ/—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ (—á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ —Ç–µ–∫—Å—Ç–∞—Ö)
            'gute', 'g√ºte', 'weise', 'weisheit', 'ruhe', 'frieden',
            'mensch', 'person', 'freund', 'kollege', 'nachbar',
            'liebe', 'trauer', 'beileid', 'kondolenz',
        }
        
        if (len(name) > 2 and 
            name_lower not in ignore_words and
            not any(word in ignore_words for word in name_lower.split())):
            
            found_names.add(name)
            
            name_words = name_lower.split()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–∞–º–∏–ª–∏—é
            if len(name_words) >= 2:
                found_first = name_words[0]
                found_last = name_words[1]
                
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∏–º—è –º–µ–º–æ—Ä–∏–∞–ª–∞, –Ω–æ —Å –¥—Ä—É–≥–æ–π —Ñ–∞–º–∏–ª–∏–µ–π
                if found_first == first_name and found_last != last_name:
                    results['wrong_last_name_detected'] = True
                    results['detected_wrong_name'] = name
                    results['wrong_last_name_details'] = f"Expected: {last_name}, Found: {found_last}"
                
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ñ–∞–º–∏–ª–∏—é –º–µ–º–æ—Ä–∏–∞–ª–∞, –Ω–æ —Å –¥—Ä—É–≥–∏–º –∏–º–µ–Ω–µ–º
                if found_last == last_name and found_first != first_name:
                    results['wrong_first_name_detected'] = True
                    results['detected_wrong_name'] = name
                    results['wrong_first_name_details'] = f"Expected: {first_name}, Found: {found_first}"
    
    results['other_names_found'] = list(found_names)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    if results['wrong_first_name_detected'] and results['wrong_last_name_detected']:
        results['context'] = 'wrong_both_names'
    elif results['wrong_first_name_detected']:
        results['context'] = 'wrong_first_name'
    elif results['wrong_last_name_detected']:
        results['context'] = 'wrong_last_name'
    elif results['full_name_mentioned']:
        results['context'] = 'correct_name'
    elif results['first_name_mentioned'] and results['last_name_mentioned']:
        results['context'] = 'both_names_separate'
    elif results['first_name_mentioned'] and not results['last_name_mentioned']:
        results['context'] = 'partial_name_first_only'
    elif results['last_name_mentioned'] and not results['first_name_mentioned']:
        results['context'] = 'partial_name_last_only'
    elif results['other_names_found']:
        # –ü–†–û–í–ï–†–Ø–ï–ú: –µ—Å–ª–∏ –¥—Ä—É–≥–∏–µ –∏–º–µ–Ω–∞ - —ç—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —Å–ª–æ–≤–∞
        real_names = []
        for name in results['other_names_found']:
            name_lower = name.lower()
            # –ï—Å–ª–∏ –∏–º—è –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–µ (–Ω–µ –æ–±—â–µ—É–ø–æ—Ç—Ä–µ–±–∏–º–æ–µ —Å–ª–æ–≤–æ)
            if (len(name) > 3 and 
                not any(common in name_lower for common in ['g√ºte', 'G√ºte', 'weise', 'frieden', 'ruhe', 'beileid'])):
                real_names.append(name)
        
        if real_names:
            results['context'] = 'different_name'
            results['other_names_found'] = real_names  # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫
        else:
            results['context'] = 'no_name'
            results['other_names_found'] = []  # –û—á–∏—â–∞–µ–º –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –∏–º–µ–Ω–∞
    else:
        results['context'] = 'no_name'
    
    return results

def prepare_name_analysis_for_prompt(name_analysis, memorial):
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑ –∏–º—ë–Ω –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç –ò–ò.
    """
    lines = []
    
    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
    if name_analysis.get('wrong_both_names', False):
        lines.append(f"üö® KRITISCH: Text erw√§hnt komplett anderen Namen '{name_analysis.get('detected_wrong_name', 'unbekannt')}' statt '{memorial.first_name} {memorial.last_name}'!")
    
    if name_analysis['wrong_first_name_detected']:
        lines.append(f"üö® FALSCHER VORNAME: Text erw√§hnt '{name_analysis.get('detected_wrong_name', 'anderer Name')}' (erwartet: '{memorial.first_name}')")
    
    if name_analysis['wrong_last_name_detected']:
        lines.append(f"üö® FALSCHER NACHNAME: Text erw√§hnt '{name_analysis.get('detected_wrong_name', 'anderer Name')}' (erwartet Nachname: '{memorial.last_name}')")
    
    # –ó–∞—Ç–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
    elif name_analysis['full_name_mentioned']:
        lines.append(f"‚úì Korrekter Name '{memorial.first_name} {memorial.last_name}' wird erw√§hnt.")
    
    elif name_analysis['context'] == 'both_names_separate':
        lines.append(f"‚ö†Ô∏è Vorname '{memorial.first_name}' und Nachname '{memorial.last_name}' getrennt erw√§hnt.")
    
    elif name_analysis['context'] == 'partial_name_first_only':
        lines.append(f"‚ö†Ô∏è Nur Vorname '{memorial.first_name}' erw√§hnt (Nachname fehlt).")
    
    elif name_analysis['context'] == 'partial_name_last_only':
        lines.append(f"‚ö†Ô∏è Nur Nachname '{memorial.last_name}' erw√§hnt (Vorname fehlt).")
    
    # –í–ê–ñ–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –£—Ç–æ—á–Ω—è–µ–º, —á—Ç–æ "andere Namen" –º–æ–≥—É—Ç –±—ã—Ç—å –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–º–∏ –ø–æ–Ω—è—Ç–∏—è–º–∏
    if name_analysis['other_names_found'] and not (name_analysis['wrong_first_name_detected'] or name_analysis['wrong_last_name_detected']):
        lines.append(f"‚ö†Ô∏è M√∂gliche andere Namen gefunden: {', '.join(name_analysis['other_names_found'][:2])}")
        lines.append(f"   HINWEIS: K√∂nnen auch abstrakte Begriffe sein (z.B. 'Seine G√ºte', 'Ihre Weisheit')!")
    
    if name_analysis['context'] == 'no_name':
        lines.append("‚ÑπÔ∏è Kein spezifischer Name erw√§hnt (erlaubt f√ºr allgemeine Kondolenzen).")
    
    if name_analysis['context'] == 'different_name':
        lines.append("‚ö†Ô∏è M√∂gliche andere Namen erw√§hnt. BITTE PR√úFEN: Sind es echte Personennamen oder abstrakte Begriffe?")
    
    return "\n".join(lines) if lines else "Keine Namensanalyse verf√ºgbar."


def build_ai_prompt(text, memorial, name_analysis_text):
    prompt_template = """<|system|>
Du bist ein Moderator f√ºr Gedenkseiten in der Schweiz. Analysiere den folgenden Nachruf (Tribute).

MEMORIAL KONTEXT:
- Name der verstorbenen Person: {memorial_name}
- Memorial-ID: {memorial_code}

NAME-ANALYSE (NUR f√ºr Kontext):
{name_analysis}

WICHTIG ZU WISSEN:
1. "Seine G√ºte", "Ihre Weisheit", "Unser Frieden" sind KEINE Personennamen, sondern abstrakte Begriffe!
2. Allgemeine Kondolenzen ohne Namensnennung sind ERLAUBT und respektvoll.
3. W√∂rter wie "G√ºte", "Frieden", "Ruhe" sind positive Attribute, keine Personennamen.

TEXT ZU ANALYSIEREN (Deutsch, Franz√∂sisch, Italienisch oder Englisch):
"{text}"

PR√úFKRITERIEN - SOFORT ABLEHNEN wenn:
A. EXPLIZITE BELEIDIGUNGEN in JEDER Sprache:
   ‚Ä¢ Russisch: "–∂–æ–ø–∞", "—Å—É–∫–∞", "–∏–¥–∏–æ—Ç", "–¥—É—Ä–∞–∫", "—à–ª—é—Ö–∞", "–∫–æ–∑–µ–ª", "–º—É–¥–∞–∫"
   ‚Ä¢ Deutsch: "scheisse", "arschloch", "hurensohn", "wichser"
   ‚Ä¢ Englisch: "shit", "fuck", "asshole", "bitch", "motherfucker"
   ‚Ä¢ Franz√∂sisch: "merde", "putain", "connard", "salope"
   ‚Ä¢ Italienisch: "merda", "cazzo", "stronzo", "vaffanculo"

B. TIERBEZEICHNUNGEN als Beleidigung:
   ‚Ä¢ "Schwein", "Hund", "Kuh", "Affe" (nur wenn direkt auf Person bezogen)
   ‚Ä¢ Ausnahme: Offensichtlicher Name wie "Herr Schwein"

C. EXPLIZITE HASSREDE:
   ‚Ä¢ Rassistische, sexistische, homophobe √Ñu√üerungen
   ‚Ä¢ Drohungen oder Gewaltaufrufe

D. PERS√ñNLICHE DATEN:
   ‚Ä¢ Telefonnummern, Adressen, Email-Adressen

E. WERBUNG/SPAM:
   ‚Ä¢ Links, Produktnennungen, kommerzielle Inhalte

ENTSCHEIDUNGSLOGIK (in dieser Reihenfolge pr√ºfen):
1. WENN Kriterien A-C zutreffen ‚Üí REJECT (confidence 0.9+)
2. WENN Kriterien D-E zutreffen ‚Üí REJECT (confidence 0.8+)
3. WENN Text respektvoll und angemessen ‚Üí APPROVE (confidence 0.6-0.95)
4. WENN unklar/mehrdeutig ‚Üí FLAG f√ºr manuelle Pr√ºfung

BEACHTE BEI NAMEN:
- Abstrakte Begriffe (G√ºte, Weisheit, Frieden) sind KEINE Personennamen!
- Fehlende Namensnennung ist bei allgemeinen Kondolenzen OK

GIB NUR DIESES JSON-FORMAT ZUR√úCK:
{{
  "verdict": "approved_ai" | "rejected_ai" | "flag_ai",
  "confidence": 0.0 bis 1.0,
  "reasoning": "Deutsche Begr√ºndung",
  "flags": ["liste", "der", "probleme"],
  "rejection_category": "explicit_insult" | "hate_speech" | "vulgarity" | "personal_data" | "spam" | "none"
}}

Sei STRENG bei Beleidigungen, aber FAIR bei respektvollen Texten!<|end|>
<|user|>
Analysiere diesen Nachruf:<|end|>
<|assistant|>"""
    
    return prompt_template.format(
        memorial_name=f"{memorial.first_name} {memorial.last_name}",
        memorial_code=memorial.short_code,
        name_analysis=name_analysis_text,
        text=text[:2000]
    )


def parse_ai_response(ai_response, tribute_id):
    """
    –ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò, –∏–∑–≤–ª–µ–∫–∞–µ—Ç JSON.
    """
    logger.info(f"Raw AI response for {tribute_id}: {ai_response[:200]}...")
    
    # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
    cleaned_response = ai_response.replace('```json', '').replace('```', '').strip()
    
    # –ü–æ–∏—Å–∫ JSON —Å –ø–æ–º–æ—â—å—é regex
    json_match = None
    json_pattern = r'\{[^{}]*\{[^{}]*\}[^{}]*\}|{[^{}]*\}'
    matches = re.findall(json_pattern, cleaned_response, re.DOTALL)
    
    if matches:
        json_match = max(matches, key=len)
        logger.info(f"Found JSON via regex: {json_match[:100]}...")
    
    if not json_match:
        start = cleaned_response.find('{')
        end = cleaned_response.rfind('}') + 1
        if start >= 0 and end > start:
            json_match = cleaned_response[start:end]
            logger.info(f"Found JSON via start/end: {json_match[:100]}...")
    
    # –ü–∞—Ä—Å–∏–Ω–≥ JSON
    if json_match:
        try:
            return json.loads(json_match)
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e}, trying to fix...")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ—á–∏—Å—Ç–∏—Ç—å JSON
            json_match_fixed = re.sub(r',\s*}', '}', json_match)
            json_match_fixed = re.sub(r',\s*]', ']', json_match_fixed)
            
            try:
                return json.loads(json_match_fixed)
            except json.JSONDecodeError as e2:
                logger.error(f"Still JSON parse error: {e2}")
    
    # Fallback –µ—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω
    logger.warning(f"No valid JSON found in response for tribute {tribute_id}")
    return {
        "verdict": "flag_ai", 
        "confidence": 0.2,
        "reasoning": "AI did not return valid JSON format",
        "flags": ["invalid_format"],
        "name_context_note": "Parse error - manual review needed"
    }


def adjust_verdict_based_on_names(ai_result, name_analysis):
    """
    –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –≤–µ—Ä–¥–∏–∫—Ç –ò–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∏–º—ë–Ω.
    """
    
    strictness = settings.AI_MODERATION_SETTINGS.get('name_verification_strictness', 'strict')
    name_check = settings.AI_MODERATION_SETTINGS.get('name_check', {})
    
    # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –≤–µ—Ä–¥–∏–∫—Ç –∏ confidence
    original_verdict = ai_result.get('verdict', 'pending_review')
    original_confidence = ai_result.get('confidence', 0.5)
    
    if name_analysis.get('wrong_both_names', False):
        # –û–±–∞ –∏–º–µ–Ω–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ ‚Üí –æ—Ç–∫–ª–æ–Ω—è–µ–º
        ai_result['verdict'] = 'rejected_ai'
        ai_result['confidence'] = 0.2
        ai_result['flags'] = ai_result.get('flags', []) + ['wrong_both_names', 'name_mismatch']
        ai_result['reasoning'] = f"NAMENSFEHLER: Falsche Person erw√§hnt. {ai_result.get('reasoning', '')}"
    
    elif name_analysis['wrong_first_name_detected']:
        # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è ‚Üí –æ—Ç–∫–ª–æ–Ω—è–µ–º
        ai_result['verdict'] = 'rejected_ai'
        ai_result['confidence'] = 0.3
        ai_result['flags'] = ai_result.get('flags', []) + ['wrong_first_name', 'name_mismatch']
        ai_result['reasoning'] = f"NAMENSFEHLER: Falscher Vorname. {ai_result.get('reasoning', '')}"
    
    elif name_analysis['wrong_last_name_detected']:
        # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–∞–º–∏–ª–∏—è ‚Üí –æ—Ç–∫–ª–æ–Ω—è–µ–º
        ai_result['verdict'] = 'rejected_ai'
        ai_result['confidence'] = 0.3
        ai_result['flags'] = ai_result.get('flags', []) + ['wrong_last_name', 'name_mismatch']
        ai_result['reasoning'] = f"NAMENSFEHLER: Falscher Nachname. {ai_result.get('reasoning', '')}"
    
    elif name_analysis['full_name_mentioned']:
        # –ü–æ–ª–Ω–æ–µ –∏–º—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ ‚Üí –ø–æ–≤—ã—à–∞–µ–º confidence
        ai_result['confidence'] = min(original_confidence * 1.1, 0.95)
        ai_result['flags'] = ai_result.get('flags', []) + ['correct_full_name']
    
    elif name_analysis['context'] == 'both_names_separate':
        # –û–±–∞ –∏–º–µ–Ω–∏ —É–ø–æ–º—è–Ω—É—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ ‚Üí –Ω–µ–º–Ω–æ–≥–æ –ø–æ–≤—ã—à–∞–µ–º confidence
        ai_result['confidence'] = min(original_confidence * 1.05, 0.95)
        ai_result['flags'] = ai_result.get('flags', []) + ['both_names_separate']
    
    elif name_analysis['context'] == 'partial_name_first_only':
        # –¢–æ–ª—å–∫–æ –∏–º—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ ‚Üí –û–î–û–ë–†–Ø–ï–ú
        if original_verdict == 'approved_ai':
            ai_result['confidence'] = min(original_confidence * 1.05, 0.95)
        ai_result['flags'] = ai_result.get('flags', []) + ['correct_first_name_only']
    
    elif name_analysis['context'] == 'partial_name_last_only':
        # –¢–æ–ª—å–∫–æ —Ñ–∞–º–∏–ª–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ ‚Üí –û–î–û–ë–†–Ø–ï–ú
        if original_verdict == 'approved_ai':
            ai_result['confidence'] = min(original_confidence * 1.05, 0.95)
        ai_result['flags'] = ai_result.get('flags', []) + ['correct_last_name_only']
    
    # === –î–†–£–ì–ò–ï –ò–ú–ï–ù–ê ‚Üí –û–¢–ö–õ–û–ù–Ø–ï–ú ===
    
    elif name_analysis['context'] == 'different_name':
        # –£–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –¥—Ä—É–≥–∏–µ –∏–º–µ–Ω–∞ ‚Üí –æ—Ç–∫–ª–æ–Ω—è–µ–º
        ai_result['verdict'] = 'rejected_ai'
        ai_result['confidence'] = 0.4
        ai_result['flags'] = ai_result.get('flags', []) + ['different_name_mentioned']
        ai_result['reasoning'] = f"NAMENSFEHLER: Text bezieht sich auf andere Person. {ai_result.get('reasoning', '')}"
    
    # === –ë–ï–ó –ò–ú–ï–ù–ò ‚Üí –û–°–¢–ê–í–õ–Ø–ï–ú –ö–ê–ö –ï–°–¢–¨ (AI —Ä–µ—à–∞–µ—Ç) ===
    
    elif name_analysis['context'] == 'no_name':
        # –ò–º—è –Ω–µ —É–ø–æ–º—è–Ω—É—Ç–æ ‚Üí –ù–ï –ú–ï–ù–Ø–ï–ú –≤–µ—Ä–¥–∏–∫—Ç, —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥
        ai_result['flags'] = ai_result.get('flags', []) + ['no_name_mentioned']
        # –ú–æ–∂–Ω–æ –°–õ–ï–ì–ö–ê —Å–Ω–∏–∑–∏—Ç—å confidence –¥–ª—è –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏
        if original_verdict == 'approved_ai':
            ai_result['confidence'] = original_confidence * 0.9
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ª–æ–≥–æ–≤
    ai_result['name_context'] = name_analysis['context']
    
    logger.info(f"Name adjustment: {name_analysis['context']}, final verdict: {ai_result['verdict']}, confidence: {ai_result['confidence']}")
    return ai_result


def check_explicit_insults(text):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —è–≤–Ω—ã–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –≤ AI"""
    text_lower = text.lower()
    
    # –Ø–≤–Ω—ã–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö
    explicit_insults = {
        'russian': ['–∂–æ–ø–∞', '—Å—É–∫–∞', '–ø–∏–∑–¥–∞', '–±–ª—è–¥—å', '–µ–±–∞—Ç—å', '—Ö—É–π', '–∏–¥–∏–æ—Ç', '–¥—É—Ä–∞–∫', '—à–ª—é—Ö–∞', '—à–ª—é—Ö', '–¥—É—Ä–∞', '–∏–¥–∏–æ—Ç–∫–∞', '—Ö—É–π–Ω—è', '–±–ª—è–¥–∏–Ω–∞'],
        'german': ['scheisse', 'arsch', 'hurensohn', 'wichser', 'fotze', 'mistst√ºck', 'schwanz', 'hass', 'huren', 'hasse', 'schwein', 'hund', 'sau', 'kuh', 'affe', 'ratte', 'k√§fer', 'dick'],
        'english': ['shit', 'fuck', 'asshole', 'bitch', 'motherfucker', 'cunt'],
        'french': ['merde', 'putain', 'connard', 'salope', 'encul√©'],
        'italian': ['merda', 'cazzo', 'stronzo', 'vaffanculo', 'puttana']
    }
    
    found_insults = []
    for language, words in explicit_insults.items():
        for word in words:
            if word in text_lower:
                found_insults.append(f"{word} ({language})")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∂–∏–≤–æ—Ç–Ω—ã—Ö –∫–∞–∫ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏–π
    animal_insults = ['schwein', 'hund', 'sau', 'kuh', 'affe', 'ratte']
    # –ù–æ –∏—Å–∫–ª—é—á–∞–µ–º, –µ—Å–ª–∏ —ç—Ç–æ —á–∞—Å—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    if 'schwein' in text_lower:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç: "Du Schwein!" vs "Herr Schwein"
        if re.search(r'\b(du|sie|er|sie)\s+schwein\b', text_lower, re.IGNORECASE):
            found_insults.append("schwein (animal_insult)")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–º–µ—Ü–∫–∏—Ö –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏–π
    additional_german = ['hass', 'hasse', 'hassen', 'idiot', 'depp', 'bl√∂d', 'dumm']
    for word in additional_german:
        if word in text_lower:
            found_insults.append(f"{word} (german_insult)")

    return found_insults    

# –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ Celery –¥–ª—è –º–æ–¥–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∏–±—å—é—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò
@shared_task
def moderate_tribute_with_ai(tribute_id, retry_count=0):
    """
    –§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –º–æ–¥–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∏–±—å—é—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò
    """
    try:
        tribute = Tribute.objects.get(id=tribute_id)
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —è–≤–Ω—ã–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è
        explicit_insults = check_explicit_insults(tribute.text)

        if explicit_insults:
            logger.warning(f"Explicit insults detected for tribute {tribute_id}: {explicit_insults}")
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª–æ–Ω—è–µ–º –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —è–≤–Ω—ã–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è
            tribute.status = 'rejected'
            tribute.ai_verdict = 'rejected_ai'
            tribute.ai_confidence = 0.95
            tribute.ai_moderation_result = {
                "verdict": "rejected_ai",
                "confidence": 0.95,
                "reasoning": f"Explizite Beleidigungen gefunden: {', '.join(explicit_insults[:3])}",
                "flags": ["explicit_insult"] + explicit_insults[:3],
                "auto_action": True,
                "rejection_category": "explicit_insult"
            }
            tribute.save()
            
            return f"Auto-rejected for explicit insults: {explicit_insults[:2]}"
        # –ï—Å–ª–∏ —É–∂–µ –æ—Ç–º–æ–¥–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏–ª–∏ –Ω–µ –≤ pending - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if tribute.status != 'pending' or tribute.ai_moderated_at:
            return f"Tribute {tribute_id} already moderated or not pending"
        
        memorial = tribute.memorial
        
        # ===== 1. –ê–ù–ê–õ–ò–ó –ò–ú–Å–ù =====
        name_analysis = analyze_name_mentions(tribute.text, memorial)
        name_analysis_text = prepare_name_analysis_for_prompt(name_analysis, memorial)
        
        logger.info(f"Name analysis for tribute {tribute_id}: {name_analysis['context']}")
        
        # ===== 2. –ü–û–°–¢–†–û–ï–ù–ò–ï –ü–†–û–ú–ü–¢–ê –° –ö–û–ù–¢–ï–ö–°–¢–û–ú =====
        prompt = build_ai_prompt(tribute.text, memorial, name_analysis_text)
        
        # ===== 3. –û–¢–ü–†–ê–í–ö–ê –í –ò–ò =====
        ollama_url = getattr(settings, 'OLLAMA_API_URL', 'http://localhost:11434/api/generate')
        model_name = getattr(settings, 'OLLAMA_MODEL', 'llama3.2:latest')
        
        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.1,
                "top_p": 0.9,
                "num_predict": 600,  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
                "stop": ["<|end|>", "\n\n"]
            }
        }
        
        try:
            response = requests.post(ollama_url, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            ai_response = result.get('response', '').strip()
            
            # ===== 4. –ü–ê–†–°–ò–ù–ì –û–¢–í–ï–¢–ê =====
            ai_result = parse_ai_response(ai_response, tribute_id)
            
            # ===== 5. –ö–û–†–†–ï–ö–¶–ò–Ø –ù–ê –û–°–ù–û–í–ï –ò–ú–Å–ù =====
            ai_result = adjust_verdict_based_on_names(ai_result, name_analysis)

            # ===== –î–û–ë–ê–í–õ–ï–ù–ù–´–ô –û–¢–õ–ê–î–û–ß–ù–´–ô –õ–û–ì =====
            logger.info(f"=== DEBUG AI MODERATION ===")
            logger.info(f"Tribute ID: {tribute_id}")
            logger.info(f"Text preview: {tribute.text[:100]}...")
            logger.info(f"AI raw response: {ai_response[:200]}...")
            logger.info(f"Parsed AI result: {ai_result}")
            logger.info(f"AI verdict: {ai_result.get('verdict')}")
            logger.info(f"AI confidence: {ai_result.get('confidence')}")
            logger.info(f"Name context: {name_analysis['context']}")
            logger.info(f"Explicit insults found earlier: {explicit_insults}")
            logger.info(f"=== END DEBUG ===")
            
            # ===== 6. –î–û–ë–ê–í–õ–ï–ù–ò–ï –ö–û–ù–¢–ï–ö–°–¢–ê –î–õ–Ø –õ–û–ì–û–í =====
            ai_result['name_context'] = name_analysis['context']
            if name_analysis['other_names_found']:
                ai_result['other_names'] = name_analysis['other_names_found'][:3]

            # ===== –î–û–ë–ê–í–õ–ï–ù–ù–´–ô –õ–û–ì –ü–ï–†–ï–î –í–´–ó–û–í–û–ú =====
            logger.info(f"Calling apply_ai_verdict with: {ai_result}")
            # ===== 7. –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–ê =====
            action = tribute.apply_ai_verdict(ai_result)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—Ö
            logger.info(f"–ò–ò –æ—Ç–º–æ–¥–µ—Ä–∏—Ä–æ–≤–∞–ª —Ç—Ä–∏–±—å—é—Ç {tribute_id}: {action}")
            logger.info(f"Name context: {name_analysis['context']}")
            
            return f"AI moderation completed for {tribute_id}: {action} (name context: {name_analysis['context']})"
            
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return handle_ollama_error(e, tribute, tribute_id, retry_count)
            
    except Tribute.DoesNotExist:
        logger.error(f"–¢—Ä–∏–±—å—é—Ç {tribute_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return f"Tribute {tribute_id} not found"
    except Exception as e:
        logger.exception(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ moderate_tribute_with_ai: {e}")
        return f"Unexpected error: {str(e)}"


def handle_ollama_error(error, tribute, tribute_id, retry_count):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama.
    """
    # Fallback –Ω–∞ —Ñ–ª–∞–≥, –µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
    if settings.DEBUG:
        logger.info(f"DEBUG: Using fallback for tribute {tribute_id}")
        
        # –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ fallback —Å —É—á—ë—Ç–æ–º –∏–º—ë–Ω
        text_lower = tribute.text.lower()
        memorial_name = f"{tribute.memorial.first_name} {tribute.memorial.last_name}".lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–º–µ–Ω–∏ –º–µ–º–æ—Ä–∏–∞–ª–∞
        name_in_text = memorial_name in text_lower
        
        if any(word in text_lower for word in ['scheisse', 'hurensohn', 'arsch', 'idiot', 'hass', 'hassen']):
            ai_result = {
                "verdict": "rejected_ai",
                "confidence": 0.92,
                "reasoning": "Enth√§lt unangemessene Sprache",
                "flags": ["inappropriate_language"],
                "name_context_note": f"Name {'erw√§hnt' if name_in_text else 'nicht erw√§hnt'}"
            }
        elif any(word in text_lower for word in ['@', 'http', 'www.', '.com', 'telefon', 'nummer']):
            ai_result = {
                "verdict": "flag_ai", 
                "confidence": 0.7,
                "reasoning": "M√∂gliche pers√∂nliche Daten oder Links",
                "flags": ["possible_personal_data"],
                "name_context_note": f"Name {'erw√§hnt' if name_in_text else 'nicht erw√§hnt'}"
            }
        elif len(text_lower) < 20:
            ai_result = {
                "verdict": "flag_ai",  
                "confidence": 0.6,
                "reasoning": "Text zu kurz f√ºr Analyse",
                "flags": ["short_text"],
                "name_context_note": f"Name {'erw√§hnt' if name_in_text else 'nicht erw√§hnt'}"
            }
        else:
            ai_result = {
                "verdict": "approved_ai",
                "confidence": 0.88 if name_in_text else 0.75,  # –ù–∏–∂–µ confidence –µ—Å–ª–∏ –∏–º—è –Ω–µ —É–ø–æ–º—è–Ω—É—Ç–æ
                "reasoning": "Text respektvoll und angemessen",
                "flags": [],
                "name_context_note": f"Name {'erw√§hnt' if name_in_text else 'nicht erw√§hnt'}"
            }
        
        action = tribute.apply_ai_verdict(ai_result)
        return f"DEBUG fallback used for {tribute_id}: {action}"
    
    # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞
    if retry_count < 3:
        moderate_tribute_with_ai.apply_async(
            args=[tribute_id, retry_count + 1],
            countdown=30 * (retry_count + 1)
        )
        return f"Retry scheduled for {tribute_id}"
    
    # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
    logger.error(f"Failed after retries for tribute {tribute_id}: {error}")
    
    tribute.ai_verdict = 'error_ai'
    tribute.ai_moderation_result = {"error": str(error)}
    tribute.save()
    
    return f"Failed after retries for {tribute_id}"