import json
import requests
import re
import logging
from django.conf import settings
from celery import shared_task
from django.utils import timezone
from .models import Tribute

logger = logging.getLogger(__name__)

# –ê–Ω–∞–ª–∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏–º–µ–Ω–∏ –º–µ–º–æ—Ä–∏–∞–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ
def analyze_name_mentions(text, memorial):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–º–µ–Ω–∏ –º–µ–º–æ—Ä–∏–∞–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞.
    """
    text_lower = text.lower()
    full_name = f"{memorial.first_name} {memorial.last_name}".lower()
    first_name = memorial.first_name.lower()
    last_name = memorial.last_name.lower()
    
    results = {
        'full_name_mentioned': full_name in text_lower,
        'first_name_mentioned': first_name in text_lower,
        'last_name_mentioned': last_name in text_lower,
        'other_names_found': [],
        'wrong_first_name_detected': False,
        'wrong_last_name_detected': False,
        'context': 'unknown'
    }

    # –®–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ "–ò–º—è –§–∞–º–∏–ª–∏—è" –∏–ª–∏ "–§–∞–º–∏–ª–∏—è"
    name_patterns = [
        r'\b([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)\s+([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)\b',  
        r'\b(Herr|Frau|Mr\.|Mrs\.|Ms\.)\s+([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+(?:\s+[A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)?)\b',
    ]
    
    all_name_matches = []
    for pattern in name_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                # –î–ª—è "–ò–º—è –§–∞–º–∏–ª–∏—è"
                name_parts = [m for m in match if m and len(m) > 1]
                if len(name_parts) >= 2:
                    found_name = ' '.join(name_parts[:2])
                    all_name_matches.append(found_name)
            else:
                # –î–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                if match and len(match) > 2:
                    all_name_matches.append(match)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–∞
    found_names = set()
    for name in all_name_matches:
        name_lower = name.lower()
        
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞
        common_words = {'herr', 'frau', 'mr', 'mrs', 'ms', 'family', 'familie', 'and', 'und', 'der', 'die', 'das'}
        
        if (len(name) > 2 and 
            name_lower not in common_words):
            
            found_names.add(name)
            
            name_words = name_lower.split()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–∞–º–∏–ª–∏—é
            if len(name_words) >= 2:
                found_first = name_words[0]
                found_last = name_words[1]
                
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∏–º—è –º–µ–º–æ—Ä–∏–∞–ª–∞, –Ω–æ —Å –¥—Ä—É–≥–æ–π —Ñ–∞–º–∏–ª–∏–µ–π
                if found_first == first_name and found_last != last_name:
                    results['wrong_last_name_detected'] = True
                    results['detected_wrong_name'] = name
                    results['wrong_last_name_details'] = f"Expected: {last_name}, Found: {found_last}"
                
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ñ–∞–º–∏–ª–∏—é –º–µ–º–æ—Ä–∏–∞–ª–∞, –Ω–æ —Å –¥—Ä—É–≥–∏–º –∏–º–µ–Ω–µ–º
                if found_last == last_name and found_first != first_name:
                    results['wrong_first_name_detected'] = True
                    results['detected_wrong_name'] = name
                    results['wrong_first_name_details'] = f"Expected: {first_name}, Found: {found_first}"
    
    results['other_names_found'] = list(found_names)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    if results['wrong_first_name_detected'] and results['wrong_last_name_detected']:
        results['context'] = 'wrong_both_names'
    elif results['wrong_first_name_detected']:
        results['context'] = 'wrong_first_name'
    elif results['wrong_last_name_detected']:
        results['context'] = 'wrong_last_name'
    elif results['full_name_mentioned']:
        results['context'] = 'correct_name'
    elif results['first_name_mentioned'] and results['last_name_mentioned']:
        results['context'] = 'both_names_separate'
    elif results['first_name_mentioned'] and not results['last_name_mentioned']:
        results['context'] = 'partial_name_first_only'
    elif results['last_name_mentioned'] and not results['first_name_mentioned']:
        results['context'] = 'partial_name_last_only'
    elif results['other_names_found']:
        results['context'] = 'different_name'
    else:
        results['context'] = 'no_name'
    
    return results

def prepare_name_analysis_for_prompt(name_analysis, memorial):
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑ –∏–º—ë–Ω –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç –ò–ò.
    """
    lines = []
    
    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
    if name_analysis.get('wrong_both_names', False):
        lines.append(f"üö® KRITISCH: Text erw√§hnt komplett anderen Namen '{name_analysis.get('detected_wrong_name', 'unbekannt')}' statt '{memorial.first_name} {memorial.last_name}'!")
    
    if name_analysis['wrong_first_name_detected']:
        lines.append(f"üö® FALSCHER VORNAME: Text erw√§hnt '{name_analysis.get('detected_wrong_name', 'anderer Name')}' (erwartet: '{memorial.first_name}')")
    
    if name_analysis['wrong_last_name_detected']:
        lines.append(f"üö® FALSCHER NACHNAME: Text erw√§hnt '{name_analysis.get('detected_wrong_name', 'anderer Name')}' (erwartet Nachname: '{memorial.last_name}')")
    
    # –ó–∞—Ç–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
    elif name_analysis['full_name_mentioned']:
        lines.append(f"‚úì Korrekter Name '{memorial.first_name} {memorial.last_name}' wird erw√§hnt.")
    
    elif name_analysis['context'] == 'both_names_separate':
        lines.append(f"‚ö†Ô∏è Vorname '{memorial.first_name}' und Nachname '{memorial.last_name}' getrennt erw√§hnt.")
    
    elif name_analysis['context'] == 'partial_name_first_only':
        lines.append(f"‚ö†Ô∏è Nur Vorname '{memorial.first_name}' erw√§hnt (Nachname fehlt).")
    
    elif name_analysis['context'] == 'partial_name_last_only':
        lines.append(f"‚ö†Ô∏è Nur Nachname '{memorial.last_name}' erw√§hnt (Vorname fehlt).")
    
    if name_analysis['other_names_found'] and not (name_analysis['wrong_first_name_detected'] or name_analysis['wrong_last_name_detected']):
        lines.append(f"‚ö†Ô∏è Andere Namen gefunden: {', '.join(name_analysis['other_names_found'][:2])}")
    
    if name_analysis['context'] == 'no_name':
        lines.append("‚ÑπÔ∏è Kein spezifischer Name erw√§hnt.")
    
    return "\n".join(lines) if lines else "Keine Namensanalyse verf√ºgbar."


def build_ai_prompt(text, memorial, name_analysis_text):
    prompt_template = """<|system|>
Du bist ein Moderator f√ºr Gedenkseiten in der Schweiz. Analysiere den folgenden Nachruf (Tribute).

MEMORIAL KONTEXT:
- Name der verstorbenen Person: {memorial_name}
- Memorial-ID: {memorial_code}

NAME-ANALYSE (f√ºr deine Ber√ºcksichtigung):
{name_analysis}

TEXT ZU ANALYSIEREN (kann auf Deutsch, Franz√∂sisch, Italienisch oder Englisch sein):
"{text}"

KATEGORISCHE ABLEHNUNGSGR√úNDE (SOFORT REJECT wenn zutreffend):
1. EXPLIZITE BELEIDIGUNGEN in JEDER Sprache (auch Russisch/andere):
   - "–∂–æ–ø–∞", "—Å—É–∫–∞", "–∏–¥–∏–æ—Ç", "–¥—É—Ä–∞–∫" (Russisch)
   - "scheisse", "arschloch", "hurensohn" (Deutsch)
   - "bitch", "asshole", "motherfucker" (Englisch)
   - "connard", "salope", "putain" (Franz√∂sisch)
   - "stronzo", "cazzo", "vaffanculo" (Italienisch)

2. TIERBEZEICHNUNGEN als Beleidigung:
   - "Schwein", "Hund", "Kuh", "Affe" wenn auf Person bezogen
   - Ausnahme: Wenn offensichtlich Name (z.B. "Herr Schwein")

3. EXPLIZITE HASSREDE:
   - Rassistische, sexistische, homophobe √Ñu√üerungen
   - Drohungen, Gewaltaufrufe

ERWEITERTE PR√úFKRITERIEN:
1. Beleidigungen, Hassrede oder Vulgarit√§t (in ALLEN Sprachen pr√ºfen!)
2. Pers√∂nliche/private Daten (Telefon, Adresse, Email)  
3. Werbung/Spam (Links, Produktnennungen)
4. Unangemessener Ton f√ºr Trauerfeier
5. TEXT-INTEGRIT√ÑT: Pl√∂tzliche Sprachwechsel, Testphrasen
6. NAMENS-KONTEXT: Bezieht sich Text auf richtiges Memorial?

ENTSCHEIDUNGSLOGIK (PRIORIT√ÑTEN):
1. WENN kategorische Ablehnungsgr√ºnde (oben) ‚Üí SOFORT REJECT (confidence 0.9+)
2. WENN offensichtliche Beleidigungen in Fremdsprachen ‚Üí REJECT
3. WENN 'Schwein', 'Hund' etc. als Beleidigung erkennbar ‚Üí REJECT
4. WENN unklarer Kontext (z.B. "Schwein" als m√∂glicher Name) ‚Üí FLAG
5. WENN respektvoll ohne Probleme ‚Üí APPROVE

BEISPIELE f√ºr SOFORT-REJECT:
- "Er war –∂–æ–ø–∞ ein Mensch" ‚Üí REJECT (–∂–æ–ø–∞ = russische Beleidigung)
- "Du Schwein!" ‚Üí REJECT (Tier als Beleidigung)
- "Scheisse, er war..." ‚Üí REJECT (explizite Vulgarit√§t)

BEISPIELE f√ºr FLAG:
- "Herr Schwein war..." ‚Üí FLAG (m√∂glicher Name)
- "Er war stark wie ein B√§r" ‚Üí APPROVE (positiver Vergleich)

GIB DEINE ANTWORT NUR IM FOLGENDEN JSON-FORMAT AUS:
{{
  "verdict": "approved_ai" | "rejected_ai" | "flag_ai",
  "confidence": 0.0 bis 1.0,
  "reasoning": "Deutsche Begr√ºndung",
  "flags": ["liste", "der", "probleme"],
  "rejection_category": "explicit_insult" | "hate_speech" | "vulgarity" | "none"
}}

WICHTIG: Sei STRENG bei Beleidigungen! Gib NUR das JSON zur√ºck, keinen zus√§tzlichen Text!<|end|>
<|user|>
Analysiere diesen Nachruf mit obigem Memorial-Kontext (Text kann Deutsch, Franz√∂sisch, Italienisch oder Englisch sein):<|end|>
<|assistant|>"""
    
    return prompt_template.format(
        memorial_name=f"{memorial.first_name} {memorial.last_name}",
        memorial_code=memorial.short_code,
        name_analysis=name_analysis_text,
        text=text[:2000]
    )


def parse_ai_response(ai_response, tribute_id):
    """
    –ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò, –∏–∑–≤–ª–µ–∫–∞–µ—Ç JSON.
    """
    logger.info(f"Raw AI response for {tribute_id}: {ai_response[:200]}...")
    
    # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
    cleaned_response = ai_response.replace('```json', '').replace('```', '').strip()
    
    # –ü–æ–∏—Å–∫ JSON —Å –ø–æ–º–æ—â—å—é regex
    json_match = None
    json_pattern = r'\{[^{}]*\{[^{}]*\}[^{}]*\}|{[^{}]*\}'
    matches = re.findall(json_pattern, cleaned_response, re.DOTALL)
    
    if matches:
        json_match = max(matches, key=len)
        logger.info(f"Found JSON via regex: {json_match[:100]}...")
    
    if not json_match:
        start = cleaned_response.find('{')
        end = cleaned_response.rfind('}') + 1
        if start >= 0 and end > start:
            json_match = cleaned_response[start:end]
            logger.info(f"Found JSON via start/end: {json_match[:100]}...")
    
    # –ü–∞—Ä—Å–∏–Ω–≥ JSON
    if json_match:
        try:
            return json.loads(json_match)
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e}, trying to fix...")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ—á–∏—Å—Ç–∏—Ç—å JSON
            json_match_fixed = re.sub(r',\s*}', '}', json_match)
            json_match_fixed = re.sub(r',\s*]', ']', json_match_fixed)
            
            try:
                return json.loads(json_match_fixed)
            except json.JSONDecodeError as e2:
                logger.error(f"Still JSON parse error: {e2}")
    
    # Fallback –µ—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω
    logger.warning(f"No valid JSON found in response for tribute {tribute_id}")
    return {
        "verdict": "flag_ai", 
        "confidence": 0.2,
        "reasoning": "AI did not return valid JSON format",
        "flags": ["invalid_format"],
        "name_context_note": "Parse error - manual review needed"
    }


def adjust_verdict_based_on_names(ai_result, name_analysis):
    """
    –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –≤–µ—Ä–¥–∏–∫—Ç –ò–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∏–º—ë–Ω.
    """
    from django.conf import settings
    
    name_settings = settings.AI_MODERATION_SETTINGS.get('name_check', {})
    strictness = settings.AI_MODERATION_SETTINGS.get('name_verification_strictness', 'moderate')
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ AI_MODERATION_SETTINGS
    auto_reject_wrong_name = name_settings.get('auto_reject_on_wrong_last_name', False)
    
    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ 
    if name_analysis.get('wrong_both_names', False):
        # –ö–æ–º–ø–ª–µ–∫—Ç–Ω–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è
        if auto_reject_wrong_name:
            ai_result['verdict'] = 'rejected_ai'
        else:
            ai_result['verdict'] = 'flag_ai'
        ai_result['confidence'] = 0.2
        ai_result['flags'] = ai_result.get('flags', []) + ['wrong_both_names', 'name_mismatch']
    
    elif name_analysis['wrong_first_name_detected']:
        # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è
        if auto_reject_wrong_name and strictness == 'strict':
            ai_result['verdict'] = 'rejected_ai'
        else:
            ai_result['verdict'] = 'flag_ai'
        ai_result['confidence'] = 0.3
        ai_result['flags'] = ai_result.get('flags', []) + ['wrong_first_name', 'name_mismatch']
    
    elif name_analysis['wrong_last_name_detected']:
        # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–∞–º–∏–ª–∏—è
        if auto_reject_wrong_name and strictness == 'strict':
            ai_result['verdict'] = 'rejected_ai'
        else:
            ai_result['verdict'] = 'flag_ai'
        ai_result['confidence'] = 0.3
        ai_result['flags'] = ai_result.get('flags', []) + ['wrong_last_name', 'name_mismatch']
        ai_result['reasoning'] = f"NAMENSFEHLER: {ai_result.get('reasoning', '')}"
    
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
    elif name_analysis['context'] == 'partial_name_first_only':
        # –¢–æ–ª—å–∫–æ –∏–º—è –±–µ–∑ —Ñ–∞–º–∏–ª–∏–∏
        if strictness == 'strict':
            ai_result['verdict'] = 'flag_ai'
            ai_result['confidence'] = ai_result.get('confidence', 0.5) * 0.6
        elif strictness == 'moderate':
            ai_result['confidence'] = ai_result.get('confidence', 0.5) * 0.7
        ai_result['flags'] = ai_result.get('flags', []) + ['missing_last_name']
    
    elif name_analysis['context'] == 'partial_name_last_only':
        # –¢–æ–ª—å–∫–æ —Ñ–∞–º–∏–ª–∏—è –±–µ–∑ –∏–º–µ–Ω–∏
        if strictness == 'strict':
            ai_result['verdict'] = 'flag_ai'
            ai_result['confidence'] = ai_result.get('confidence', 0.5) * 0.6
        elif strictness == 'moderate':
            ai_result['confidence'] = ai_result.get('confidence', 0.5) * 0.7
        ai_result['flags'] = ai_result.get('flags', []) + ['missing_first_name']
    
    elif name_analysis['context'] == 'different_name':
        # –î—Ä—É–≥–∏–µ –∏–º–µ–Ω–∞
        if strictness == 'strict':
            ai_result['verdict'] = 'flag_ai'
            ai_result['confidence'] = 0.4
        elif strictness == 'moderate':
            ai_result['confidence'] = ai_result.get('confidence', 0.5) * 0.6
        ai_result['flags'] = ai_result.get('flags', []) + ['different_name_mentioned']
    
    return ai_result


def check_explicit_insults(text):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —è–≤–Ω—ã–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –≤ AI"""
    text_lower = text.lower()
    
    # –Ø–≤–Ω—ã–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö
    explicit_insults = {
        'russian': ['–∂–æ–ø–∞', '—Å—É–∫–∞', '–ø–∏–∑–¥–∞', '–±–ª—è–¥—å', '–µ–±–∞—Ç—å', '—Ö—É–π', '–∏–¥–∏–æ—Ç', '–¥—É—Ä–∞–∫'],
        'german': ['scheisse', 'arsch', 'hurensohn', 'wichser', 'fotze', 'mistst√ºck'],
        'english': ['shit', 'fuck', 'asshole', 'bitch', 'motherfucker', 'cunt'],
        'french': ['merde', 'putain', 'connard', 'salope', 'encul√©'],
        'italian': ['merda', 'cazzo', 'stronzo', 'vaffanculo', 'puttana']
    }
    
    found_insults = []
    for language, words in explicit_insults.items():
        for word in words:
            if word in text_lower:
                found_insults.append(f"{word} ({language})")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∂–∏–≤–æ—Ç–Ω—ã—Ö –∫–∞–∫ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏–π
    animal_insults = ['schwein', 'hund', 'sau', 'kuh', 'affe', 'ratte']
    # –ù–æ –∏—Å–∫–ª—é—á–∞–µ–º, –µ—Å–ª–∏ —ç—Ç–æ —á–∞—Å—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    if 'schwein' in text_lower:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç: "Du Schwein!" vs "Herr Schwein"
        if re.search(r'\b(du|sie|er|sie)\s+schwein\b', text_lower, re.IGNORECASE):
            found_insults.append("schwein (animal_insult)")
    
    return found_insults    

# –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ Celery –¥–ª—è –º–æ–¥–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∏–±—å—é—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò
@shared_task
def moderate_tribute_with_ai(tribute_id, retry_count=0):
    """
    –§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –º–æ–¥–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∏–±—å—é—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò
    """
    try:
        tribute = Tribute.objects.get(id=tribute_id)
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —è–≤–Ω—ã–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è
        explicit_insults = check_explicit_insults(tribute.text)

        if explicit_insults:
            logger.warning(f"Explicit insults detected for tribute {tribute_id}: {explicit_insults}")
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª–æ–Ω—è–µ–º –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —è–≤–Ω—ã–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è
            tribute.status = 'rejected'
            tribute.ai_verdict = 'rejected_ai'
            tribute.ai_confidence = 0.95
            tribute.ai_moderation_result = {
                "verdict": "rejected_ai",
                "confidence": 0.95,
                "reasoning": f"Explizite Beleidigungen gefunden: {', '.join(explicit_insults[:3])}",
                "flags": ["explicit_insult"] + explicit_insults[:3],
                "auto_action": True,
                "rejection_category": "explicit_insult"
            }
            tribute.save()
            
            return f"Auto-rejected for explicit insults: {explicit_insults[:2]}"
        # –ï—Å–ª–∏ —É–∂–µ –æ—Ç–º–æ–¥–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏–ª–∏ –Ω–µ –≤ pending - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if tribute.status != 'pending' or tribute.ai_moderated_at:
            return f"Tribute {tribute_id} already moderated or not pending"
        
        memorial = tribute.memorial
        
        # ===== 1. –ê–ù–ê–õ–ò–ó –ò–ú–Å–ù =====
        name_analysis = analyze_name_mentions(tribute.text, memorial)
        name_analysis_text = prepare_name_analysis_for_prompt(name_analysis, memorial)
        
        logger.info(f"Name analysis for tribute {tribute_id}: {name_analysis['context']}")
        
        # ===== 2. –ü–û–°–¢–†–û–ï–ù–ò–ï –ü–†–û–ú–ü–¢–ê –° –ö–û–ù–¢–ï–ö–°–¢–û–ú =====
        prompt = build_ai_prompt(tribute.text, memorial, name_analysis_text)
        
        # ===== 3. –û–¢–ü–†–ê–í–ö–ê –í –ò–ò =====
        ollama_url = getattr(settings, 'OLLAMA_API_URL', 'http://localhost:11434/api/generate')
        model_name = getattr(settings, 'OLLAMA_MODEL', 'phi3:latest')
        
        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.1,
                "top_p": 0.9,
                "num_predict": 600,  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
                "stop": ["<|end|>", "\n\n"]
            }
        }
        
        try:
            response = requests.post(ollama_url, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            ai_response = result.get('response', '').strip()
            
            # ===== 4. –ü–ê–†–°–ò–ù–ì –û–¢–í–ï–¢–ê =====
            ai_result = parse_ai_response(ai_response, tribute_id)
            
            # ===== 5. –ö–û–†–†–ï–ö–¶–ò–Ø –ù–ê –û–°–ù–û–í–ï –ò–ú–Å–ù =====
            ai_result = adjust_verdict_based_on_names(ai_result, name_analysis)
            
            # ===== 6. –î–û–ë–ê–í–õ–ï–ù–ò–ï –ö–û–ù–¢–ï–ö–°–¢–ê –î–õ–Ø –õ–û–ì–û–í =====
            ai_result['name_context'] = name_analysis['context']
            if name_analysis['other_names_found']:
                ai_result['other_names'] = name_analysis['other_names_found'][:3]
            
            # ===== 7. –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–ê =====
            action = tribute.apply_ai_verdict(ai_result)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—Ö
            logger.info(f"–ò–ò –æ—Ç–º–æ–¥–µ—Ä–∏—Ä–æ–≤–∞–ª —Ç—Ä–∏–±—å—é—Ç {tribute_id}: {action}")
            logger.info(f"Name context: {name_analysis['context']}")
            
            return f"AI moderation completed for {tribute_id}: {action} (name context: {name_analysis['context']})"
            
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return handle_ollama_error(e, tribute, tribute_id, retry_count)
            
    except Tribute.DoesNotExist:
        logger.error(f"–¢—Ä–∏–±—å—é—Ç {tribute_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return f"Tribute {tribute_id} not found"
    except Exception as e:
        logger.exception(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ moderate_tribute_with_ai: {e}")
        return f"Unexpected error: {str(e)}"


def handle_ollama_error(error, tribute, tribute_id, retry_count):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama.
    """
    # Fallback –Ω–∞ —Ñ–ª–∞–≥, –µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
    if settings.DEBUG:
        logger.info(f"DEBUG: Using fallback for tribute {tribute_id}")
        
        # –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ fallback —Å —É—á—ë—Ç–æ–º –∏–º—ë–Ω
        text_lower = tribute.text.lower()
        memorial_name = f"{tribute.memorial.first_name} {tribute.memorial.last_name}".lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–º–µ–Ω–∏ –º–µ–º–æ—Ä–∏–∞–ª–∞
        name_in_text = memorial_name in text_lower
        
        if any(word in text_lower for word in ['scheisse', 'hurensohn', 'arsch', 'idiot', 'hass', 'hassen']):
            ai_result = {
                "verdict": "rejected_ai",
                "confidence": 0.92,
                "reasoning": "Enth√§lt unangemessene Sprache",
                "flags": ["inappropriate_language"],
                "name_context_note": f"Name {'erw√§hnt' if name_in_text else 'nicht erw√§hnt'}"
            }
        elif any(word in text_lower for word in ['@', 'http', 'www.', '.com', 'telefon', 'nummer']):
            ai_result = {
                "verdict": "flag_ai", 
                "confidence": 0.7,
                "reasoning": "M√∂gliche pers√∂nliche Daten oder Links",
                "flags": ["possible_personal_data"],
                "name_context_note": f"Name {'erw√§hnt' if name_in_text else 'nicht erw√§hnt'}"
            }
        elif len(text_lower) < 20:
            ai_result = {
                "verdict": "flag_ai",  
                "confidence": 0.6,
                "reasoning": "Text zu kurz f√ºr Analyse",
                "flags": ["short_text"],
                "name_context_note": f"Name {'erw√§hnt' if name_in_text else 'nicht erw√§hnt'}"
            }
        else:
            ai_result = {
                "verdict": "approved_ai",
                "confidence": 0.88 if name_in_text else 0.75,  # –ù–∏–∂–µ confidence –µ—Å–ª–∏ –∏–º—è –Ω–µ —É–ø–æ–º—è–Ω—É—Ç–æ
                "reasoning": "Text respektvoll und angemessen",
                "flags": [],
                "name_context_note": f"Name {'erw√§hnt' if name_in_text else 'nicht erw√§hnt'}"
            }
        
        action = tribute.apply_ai_verdict(ai_result)
        return f"DEBUG fallback used for {tribute_id}: {action}"
    
    # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞
    if retry_count < 3:
        moderate_tribute_with_ai.apply_async(
            args=[tribute_id, retry_count + 1],
            countdown=30 * (retry_count + 1)
        )
        return f"Retry scheduled for {tribute_id}"
    
    # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
    logger.error(f"Failed after retries for tribute {tribute_id}: {error}")
    
    tribute.ai_verdict = 'error_ai'
    tribute.ai_moderation_result = {"error": str(error)}
    tribute.save()
    
    return f"Failed after retries for {tribute_id}"